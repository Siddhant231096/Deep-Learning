{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "like words get streamed like intelligetnly,intelligence,intelligens  ---  all come in same domain named as intelligen  \n",
    "so we stem it and thsi process is called steamming.\n",
    "\n",
    "\n",
    "Use for spam or Ham\n",
    "\n",
    "\n",
    "This all done using NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "to analysis  the words and sentence and give its meaning full value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "to divide data in sentence or words to fetch meaning full data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Habit is to remove stopwords first then do anything in NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pyttsx3 ---- for voice output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--- wordnet is for synon. ,anto,defin,examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk import stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "data='''this is me and i know who Siddhant i am shut your mouth. I am feeling happy and want to play TT more and more i think you are idiot.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is me and i know who Siddhant i am shut your mouth.', 'I am feeling happy and want to play TT more and more i think you are idiot.']\n",
      "['this', 'is', 'me', 'and', 'i', 'know', 'who', 'Siddhant', 'i', 'am', 'shut', 'your', 'mouth', '.', 'I', 'am', 'feeling', 'happy', 'and', 'want', 'to', 'play', 'TT', 'more', 'and', 'more', 'i', 'think', 'you', 'are', 'idiot', '.']\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenize\n",
    "\n",
    "print(sent_tokenize(data[0]))\n",
    "print(word_tokenize(data[0]))\n",
    "word=word_tokenize(data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "#import pyttsx3\n",
    "counter =0\n",
    "for i in word:\n",
    "    counter=counter+1\n",
    "    if i == 'shut':\n",
    "        print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"engine=pyttsx3.init()\\nengine.say('hello')\\n\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''engine=pyttsx3.init()\n",
    "engine.say('hello')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is me and i know who Siddhant i am shut your mouth. I am feeling happy and want to play TT more and more i think you are idiot.']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removeword=[i for i in data if i.lower() not in stopwords.words('english')]\n",
    "removeword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is me and i know who Siddhant i am shut your mouth. I am feeling happy and want to play TT more and more i think you are idiot.']\n",
      "['h', ' ', ' ', 'e', ' ', 'n', ' ', ' ', 'k', 'n', 'w', ' ', 'w', 'h', ' ', 'h', 'n', ' ', ' ', ' ', 'h', 'u', ' ', 'u', 'r', ' ', 'u', 'h', '.', ' ', ' ', ' ', 'f', 'e', 'e', 'l', 'n', 'g', ' ', 'h', 'p', 'p', ' ', 'n', ' ', 'w', 'n', ' ', ' ', 'p', 'l', ' ', ' ', 'r', 'e', ' ', 'n', ' ', 'r', 'e', ' ', ' ', 'h', 'n', 'k', ' ', 'u', ' ', 'r', 'e', ' ', '.']\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(removeword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WordNetLemmatizer']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in dir(stem) if 'Lemm' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordNetLemmatizer=stem.WordNetLemmatizer\n",
    "PorterStemmer=stem.PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is me and i know who Siddhant i am shut your mouth. I am feeling happy and want to play TT more and more i think you are idiot.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['know',\n",
       " 'Siddhant',\n",
       " 'shut',\n",
       " 'mouth',\n",
       " '.',\n",
       " 'feeling',\n",
       " 'happy',\n",
       " 'want',\n",
       " 'play',\n",
       " 'TT',\n",
       " 'think',\n",
       " 'idiot',\n",
       " '.']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removeword=[i for i in word_tokenize(data) if i.lower() not in stopwords.words('english')]\n",
    "removeword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot\n"
     ]
    }
   ],
   "source": [
    "wl=stem.WordNetLemmatizer()\n",
    "print(wl.lemmatize('feet'))\n",
    "\n",
    "\n",
    "\n",
    "#ps=steam.PorterStemmer()\n",
    "#print(ps.steam('feet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
